{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T06:28:04.379662Z",
     "start_time": "2024-05-07T06:28:01.678963Z"
    }
   },
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import e3nn\n",
    "from e3nn import o3, io\n",
    "import numpy as np\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import open3d as o3d\n",
    "import plotly\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.io\n",
    "\n",
    "# plotly.io.renderers.default = \"notebook\"\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from functools import partial, reduce\n",
    "\n",
    "\n",
    "from utils import load_model, save_model, CustomLRScheduler\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device = 'cpu' # for now batch=1 so no need for gpu\n",
    "device\n"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6bf9f9eb403fc549",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T06:28:04.586544Z",
     "start_time": "2024-05-07T06:28:04.381110Z"
    }
   },
   "source": [
    "from data_generation import SimpleShapeDataset, SimpleShapeUniformRayDataset\n",
    "\n",
    "dataset = SimpleShapeUniformRayDataset(sample_points=20000)\n",
    "\n",
    "data = next(iter(DataLoader(dataset, batch_size=1))).float()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "794b8d1c7c0b5bc4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-07T06:30:08.604796Z",
     "start_time": "2024-05-07T06:30:08.201002Z"
    }
   },
   "source": [
    "# how does it look like in spherical harmonics?\n",
    "\n",
    "sphten = io.SphericalTensor(11, 1, 1)\n",
    "signal = o3.spherical_harmonics(sphten, x=data, normalize=False).mean(dim=[0, 1])\n",
    "\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'is_3d': True} for j in range(2)] for i in range(1)])\n",
    "# fig.add_trace(go.Scatter3d(x=0.12 * data[0, :, 0], y=0.12 * data[0, :, 1], z=0.12 * data[0, :, 2]), row=1, col=2)\n",
    "fig.add_trace(go.Surface(sphten.plotly_surface(signal, radius=True)[0]), row=1, col=2)\n",
    "fig.add_trace(go.Scatter3d(x=data[0, :, 0], y=data[0, :, 1], z=data[0, :, 2]), row=1, col=1)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9e0ad69bff8805f2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:34:16.052809Z",
     "start_time": "2024-05-06T02:34:16.028241Z"
    }
   },
   "source": [
    "class SelfTP(nn.Module):\n",
    "    def __init__(self, irreps_in, irreps_out, batch_norm=True):\n",
    "        super(SelfTP, self).__init__()\n",
    "        \n",
    "        self.tp = o3.FullyConnectedTensorProduct(irreps_in1=irreps_in, irreps_in2=irreps_in, irreps_out=irreps_out)\n",
    "        self.bn = e3nn.nn.BatchNorm(irreps_out)\n",
    "        self.batch_norm = batch_norm\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.tp(x, x)\n",
    "        if self.batch_norm:\n",
    "            x = self.bn(x)\n",
    "        return x      \n",
    "\n",
    "\n",
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, lmax=8, max_channel=64):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "       \n",
    "        self.irrep_layer = []\n",
    "        tw = 1\n",
    "        for l in range(lmax, 0, -1): # last one sould be lmax=1\n",
    "            irreps = io.SphericalTensor(lmax=l, p_arg=1, p_val=1)\n",
    "            irreps = irreps * tw\n",
    "            irreps = (irreps.sort())[0]\n",
    "            irreps = irreps.simplify()\n",
    "            self.irrep_layer.append(irreps)\n",
    "            tw *= 2\n",
    "            tw = min(tw, max_channel) # todo tune the max channel later\n",
    "\n",
    "        self.model_sphten_repr = io.SphericalTensor(lmax=lmax, p_arg=1, p_val=1)\n",
    "        self.latent_repr = self.irrep_layer[-1]\n",
    "        \n",
    "        # todo should the last layer have batch norm?\n",
    "        # todo use fully connected tensor product?\n",
    "        self.encoder = nn.Sequential(*[\n",
    "            SelfTP(irreps_in=self.irrep_layer[i], irreps_out=self.irrep_layer[i+1], batch_norm=(i != len(self.irrep_layer)-2))\n",
    "            for i in range(len(self.irrep_layer)-1)\n",
    "        ])\n",
    "\n",
    "        # todo is the information bottleneck limited enough?\n",
    "        self.decoder = nn.Sequential(*[\n",
    "            SelfTP(irreps_in=self.irrep_layer[i+1], irreps_out=self.irrep_layer[i], batch_norm=(i != 0))\n",
    "            for i in range(len(self.irrep_layer)-2, -1, -1)            \n",
    "        ])\n",
    "\n",
    "    # def forward(self, points):\n",
    "    #     inp = o3.spherical_harmonics(l=self.model_sphten_repr, x=points, normalize=False).mean(dim=1) # todo fix the normalization later\n",
    "    #     latent = self.encoder(inp)\n",
    "    #     out = self.decoder(latent)\n",
    "    #     return inp, latent, out\n",
    "\n",
    "    def forward(self, inp):\n",
    "        # inp = o3.spherical_harmonics(l=self.model_sphten_repr, x=points, normalize=False).mean(dim=1) # todo fix the normalization later\n",
    "        latent = self.encoder(inp)\n",
    "        out = self.decoder(latent)\n",
    "        return inp, latent, out"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6295a867d6b80250",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:34:30.640568Z",
     "start_time": "2024-05-06T02:34:19.916750Z"
    }
   },
   "source": [
    "model = EncoderDecoder().to(device)\n",
    "\n",
    "print(\"number of parameters = \", sum([np.prod(x.shape) for x in model.parameters()]))\n",
    "\n",
    "dataset = SimpleShapeUniformRayDataset(sample_points=20000)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97a44d7a7626c102",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:34:30.866268Z",
     "start_time": "2024-05-06T02:34:30.641972Z"
    }
   },
   "source": [
    "shs = []\n",
    "\n",
    "for i in range(len(dataset)):\n",
    "    points = torch.from_numpy(dataset[i])\n",
    "    sh = o3.spherical_harmonics(l=model.model_sphten_repr, x=points, normalize=False).mean(dim=0)\n",
    "    shs.append(sh)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a49ebffebb64ae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:34:30.894381Z",
     "start_time": "2024-05-06T02:34:30.867053Z"
    }
   },
   "source": [
    "def train(\n",
    "        model,\n",
    "        dataset,\n",
    "        loss_fn,        \n",
    "        epochs=600,\n",
    "        checkpoint_interval=50,\n",
    "        initial_rl=1,\n",
    "        scheduler=None,\n",
    "        optimizer=None\n",
    "):\n",
    "    \n",
    "    if optimizer is None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=initial_rl)\n",
    "\n",
    "    dataloader = DataLoader(dataset, batch_size=len(dataset))\n",
    "\n",
    "    pbar = tqdm(range(epochs))\n",
    "    losses = []\n",
    "    for epoch in pbar:\n",
    "        for data in dataloader:\n",
    "            data = data.float().to(device)            \n",
    "            inp, latent, out = model(data)    \n",
    "            loss = loss_fn(inp, out)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            max_grad = max([torch.linalg.norm(p.grad).item() for p in model.parameters() if p.grad is not None])\n",
    "            optimizer.step()\n",
    "            pbar.set_description(f'Epoch {i+1} Loss: {loss.item():.8f} Lr={optimizer.param_groups[0][\"lr\"]} MaxGrad={max_grad:.4f}')\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "        if epoch % checkpoint_interval == 0:\n",
    "            name = f'epoch_{epoch}'\n",
    "            save_model(model, name)\n",
    "            print(f'saved {name}')\n",
    "\n",
    "        if scheduler is not None:   \n",
    "            scheduler.step()\n",
    "    return losses"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "660366e2d55e17fb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-05T21:13:03.464873Z",
     "start_time": "2024-05-05T21:13:03.368656Z"
    }
   },
   "source": [
    "save_model(model, 'initial_state')\n",
    "# load_model(model, 'sample')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bb84db13b3c5848a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:35:20.805206Z",
     "start_time": "2024-05-06T02:35:20.528789Z"
    }
   },
   "source": [
    "initial_rl = 1\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_rl)\n",
    "custom_scheduler = CustomLRScheduler(optimizer, initial_rl)\n",
    "# scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[100, 200, 300, 400, 500], gamma=1.1) # todo interesting apparently you shouldn't change rl\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9622d682-aa36-46fc-a51d-2824f2fc3734",
   "metadata": {},
   "source": [
    "losses = train(model=model, dataset=shs, loss_fn=loss_fn, epochs=100, initial_rl=None, scheduler=custom_scheduler, optimizer=optimizer)\n",
    "save_model(model, 'final_state')\n",
    "\n",
    "plt.plot(losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "a65a000f-1687-4373-9253-85a4582ebda6",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "load_model(model, 'initial_state')\n",
    "\n",
    "initial_rl = 0.01\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=initial_rl)\n",
    "custom_scheduler = CustomLRScheduler(optimizer, initial_rl)\n",
    "\n",
    "losses = train(model=model, dataset=shs, loss_fn=loss_fn, epochs=100, initial_rl=None, scheduler=custom_scheduler, optimizer=optimizer)\n",
    "save_model(model, 'final_state')\n",
    "\n",
    "plt.plot(losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e01d136f-2fa1-4772-88f4-78d70fd4fd46",
   "metadata": {},
   "source": [
    "load_model(model, 'final_state')\n",
    "save_model(model, 'almost_perfect_state')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3cf80305-6340-4bef-8850-fed90a85278f",
   "metadata": {},
   "source": [
    "# optimizer = torch.optim.Adam(model.parameters(), lr=initial_rl)\n",
    "# custom_scheduler = CustomLRScheduler(optimizer, initial_rl)\n",
    "\n",
    "custom_scheduler.set_rl(0.01)\n",
    "losses = train(model=model, dataset=shs, loss_fn=loss_fn, epochs=100, initial_rl=None, scheduler=custom_scheduler, optimizer=optimizer)\n",
    "save_model(model, 'final_state')\n",
    "\n",
    "plt.plot(losses)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9c8ad9d-f3bf-4542-aaf0-2b1e6270e8a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:35:26.007881Z",
     "start_time": "2024-05-06T02:35:24.031539Z"
    }
   },
   "source": [
    "load_model(model, 'almost_perfect_state')\n",
    "\n",
    "dataloader = DataLoader(shs, batch_size=len(shs)) # todo note that this is shs \n",
    "\n",
    "single_data = next(iter(dataloader)).float().to(device)\n",
    "\n",
    "inp, latent, out = model(single_data)\n",
    "inp = inp.cpu().detach()\n",
    "latent = latent.cpu().detach()\n",
    "out = out.cpu().detach()\n",
    "\n",
    "print('loss', loss_fn(inp, out))\n",
    "print('L-inf loss', torch.max(torch.abs(inp - out)))\n",
    "\n",
    "sphten = model.model_sphten_repr\n",
    "fig = make_subplots(rows=1, cols=2, specs=[[{'is_3d': True} for j in range(2)] for i in range(1)])\n",
    "fig.add_trace(go.Surface(sphten.plotly_surface(out, radius=True)[0]), row=1, col=1)\n",
    "fig.add_trace(go.Surface(sphten.plotly_surface(inp, radius=True)[0]), row=1, col=2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "691baf1a7147d1e3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:05.180127Z",
     "start_time": "2024-05-06T02:23:05.151824Z"
    }
   },
   "source": [
    "inp"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "210c50e4b2f57a06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:07.400305Z",
     "start_time": "2024-05-06T02:23:07.374294Z"
    }
   },
   "source": [
    "out"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "61c8012f-b74b-4a41-8621-8d382c2abf99",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:09.155899Z",
     "start_time": "2024-05-06T02:23:09.130322Z"
    }
   },
   "source": [
    "latent"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "91567b9b9d6d683e",
   "metadata": {},
   "source": [
    "### Test Rotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d866d736ce635064",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:13.199501Z",
     "start_time": "2024-05-06T02:23:13.173688Z"
    }
   },
   "source": [
    "pc1 = dataset[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d1347ad18d87c600",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:28.883480Z",
     "start_time": "2024-05-06T02:23:28.856125Z"
    }
   },
   "source": [
    "# todo later try with a different object\n",
    "\n",
    "pc2 = np.einsum('ij,nj->ni', o3.rand_matrix(), dataset[0])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "4c5a2356580a4ef8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:33.551836Z",
     "start_time": "2024-05-06T02:23:33.374266Z"
    }
   },
   "source": [
    "fig = make_subplots(rows=1, cols=2, specs=[[{'is_3d': True} for j in range(2)] for i in range(1)])\n",
    "fig.add_trace(go.Scatter3d(x=pc1[:, 0], y=pc1[:, 1], z=pc1[:, 2]), row=1, col=1)\n",
    "fig.add_trace(go.Scatter3d(x=pc2[:, 0], y=pc2[:, 1], z=pc2[:, 2]), row=1, col=2)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b6bc0370328c736",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:38.950273Z",
     "start_time": "2024-05-06T02:23:38.866935Z"
    }
   },
   "source": [
    "load_model(model, 'almost_perfect_state')\n",
    "model.eval()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7c372063178ccf1a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:43.013130Z",
     "start_time": "2024-05-06T02:23:42.701008Z"
    }
   },
   "source": [
    "sh1 = o3.spherical_harmonics(l=model.model_sphten_repr, x=torch.from_numpy(pc1).float().unsqueeze(0), normalize=False).mean(dim=1)\n",
    "sh2 = o3.spherical_harmonics(l=model.model_sphten_repr, x=torch.from_numpy(pc2).float().unsqueeze(0), normalize=False).mean(dim=1)\n",
    "\n",
    "with torch.no_grad():\n",
    "    inp1, latent1, out1 = model(sh1)\n",
    "    inp2, latent2, out2 = model(sh2)\n",
    "\n",
    "def interpolate_in_latent_space(repr, latent1, latent2, s):\n",
    "    res = torch.empty_like(latent1)\n",
    "    assert(latent1.shape == latent2.shape)\n",
    "    \n",
    "    ind = 0\n",
    "    for l in repr.ls:\n",
    "        sz = 2 * l + 1\n",
    "        vec1 = latent1[..., ind:ind+sz]\n",
    "        vec2 = latent2[..., ind:ind+sz]\n",
    "        \n",
    "        if sz == 1:\n",
    "            # todo we do linear interpolation because the signs might be different... why do we not have to care about this in other irreps?\n",
    "            vec_interp = vec1 + (vec2 - vec1) * s\n",
    "            res[..., ind:ind+sz] = vec_interp\n",
    "        elif sz == 3:\n",
    "            norm1 = torch.linalg.norm(vec1, dim=-1, keepdim=True)\n",
    "            norm2 = torch.linalg.norm(vec2, dim=-1, keepdim=True)\n",
    "            vec1_norm = vec1 / norm1\n",
    "            vec2_norm = vec2 / norm2\n",
    "            axis = torch.cross(vec1_norm, vec2_norm, dim=-1)\n",
    "            axis_norm = torch.norm(axis, dim=-1)\n",
    "            axis /= axis_norm\n",
    "            theta = torch.asin(axis_norm)\n",
    "            R = o3.axis_angle_to_matrix(axis=axis, angle=theta * s)\n",
    "            \n",
    "            vec_interp_size = vec1 * torch.pow(norm2 / norm1, s)\n",
    "            vec_interp_rotated = torch.einsum('nij,nj->ni', R, vec_interp_size)\n",
    "            vec_interp = vec_interp_rotated\n",
    "            res[..., ind:ind+sz] = vec_interp\n",
    "        else:\n",
    "            raise Exception(\"l > 1 is not supported in interpolation sorry :))\")\n",
    "\n",
    "        ind += sz\n",
    "    assert(ind == res.shape[-1])\n",
    "    return res\n",
    "\n",
    "\n",
    "# interpolate_in_latent_space(repr=o3.Irreps('1e') + o3.Irrep('0e'), latent1=torch.tensor([10, 0, 0, 0]).float().unsqueeze(0), latent2=torch.tensor([1, -1, 0, 10]).float().unsqueeze(0), s=1)\n",
    "# torch.max(torch.abs(interpolate_in_latent_space(repr=model.latent_repr, latent1=latent1, latent2=latent2, s=1) - latent2))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f7889bc05f920b4a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:23:58.700038Z",
     "start_time": "2024-05-06T02:23:58.243018Z"
    }
   },
   "source": [
    "N = 3\n",
    "cnt = 0\n",
    "rows = 1\n",
    "columns = 4\n",
    "\n",
    "fig = make_subplots(rows=rows, cols=columns, specs=[[{'is_3d': True} for j in range(columns)] for i in range(rows)])\n",
    "\n",
    "for i in range(rows):\n",
    "    for j in range(columns):\n",
    "        if cnt >= N:\n",
    "            continue\n",
    "        latent_interp = interpolate_in_latent_space(repr=model.latent_repr, latent1=latent1, latent2=latent2, s=cnt/N)\n",
    "        with torch.no_grad():\n",
    "            out_interp = model.decoder(latent_interp).detach().cpu()\n",
    "        sphten = model.model_sphten_repr\n",
    "        fig.add_trace(go.Surface(sphten.plotly_surface(out_interp, radius=True)[0]), row=i+1, col=j+1)\n",
    "        cnt += 1\n",
    "\n",
    "fig.add_trace(go.Surface(sphten.plotly_surface(out_interp, radius=True)[0]), row=1, col=4)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d9ee458af6ac5f83",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-06T02:30:12.020468Z",
     "start_time": "2024-05-06T02:30:11.796547Z"
    }
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
